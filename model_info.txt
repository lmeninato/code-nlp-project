summary(encoder, input_data=[code_input, code_padding_mask])

===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
TransformerEncoderModel                       [513, 16, 256]            --
├─Embedding: 1-1                              [513, 16, 256]            12,877,056
├─PositionalEncoding: 1-2                     [513, 16, 256]            --
│    └─Dropout: 2-1                           [513, 16, 256]            --
├─TransformerEncoder: 1-3                     [513, 16, 256]            --
│    └─ModuleList: 2-2                        --                        --
│    │    └─TransformerEncoderLayer: 3-1      [513, 16, 256]            527,104
│    │    └─TransformerEncoderLayer: 3-2      [513, 16, 256]            527,104
===============================================================================================
Total params: 13,931,264
Trainable params: 13,931,264
Non-trainable params: 0
Total mult-adds (G): 6.88
===============================================================================================
Input size (MB): 0.10
Forward/backward pass size (MB): 184.91
Params size (MB): 53.62
Estimated Total Size (MB): 238.63
===============================================================================================

summary(decoder_docstring, input_data=[docstring_input, code_representation, docstring_mask])

===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
TransformerDecoderModel                       [512, 16, 50257]          --
├─Embedding: 1-1                              [512, 16, 256]            12,865,792
├─PositionalEncoding: 1-2                     [512, 16, 256]            --
│    └─Dropout: 2-1                           [512, 16, 256]            --
├─TransformerDecoder: 1-3                     [512, 16, 256]            --
│    └─ModuleList: 2-2                        --                        --
│    │    └─TransformerDecoderLayer: 3-1      [512, 16, 256]            790,784
│    │    └─TransformerDecoderLayer: 3-2      [512, 16, 256]            790,784
├─Linear: 1-4                                 [512, 16, 50257]          12,916,049
===============================================================================================
Total params: 27,363,409
Trainable params: 27,363,409
Non-trainable params: 0
Total mult-adds (G): 13.47
===============================================================================================
Input size (MB): 9.52
Forward/backward pass size (MB): 3511.75
Params size (MB): 105.24
Estimated Total Size (MB): 3626.51
===============================================================================================


summary(decoder_code, input_data=[code_input, code_representation, code_mask, lang_token_id])

===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
TransformerDecoderModel                       [513, 16, 50301]          --
├─Embedding: 1-1                              [513, 16, 256]            12,877,056
├─Embedding: 1-2                              [16, 1, 256]              (recursive)
├─PositionalEncoding: 1-3                     [513, 16, 256]            --
│    └─Dropout: 2-1                           [513, 16, 256]            --
├─TransformerDecoder: 1-4                     [513, 16, 256]            --
│    └─ModuleList: 2-2                        --                        --
│    │    └─TransformerDecoderLayer: 3-1      [513, 16, 256]            790,784
│    │    └─TransformerDecoderLayer: 3-2      [513, 16, 256]            790,784
├─Linear: 1-5                                 [513, 16, 50301]          12,927,357
===============================================================================================
Total params: 27,385,981
Trainable params: 27,385,981
Non-trainable params: 0
Total mult-adds (G): 13.72
===============================================================================================
Input size (MB): 9.52
Forward/backward pass size (MB): 3521.53
Params size (MB): 105.33
Estimated Total Size (MB): 3636.38
===============================================================================================
